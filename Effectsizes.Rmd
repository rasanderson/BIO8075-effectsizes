---
title: "p-values, NHST, effect sizes"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(mosaic)
library(metafor)
library(shiny)
knitr::opts_chunk$set(echo = FALSE)
```


## p-values, Effect sizes

### 95% Confidence intervals 

We have already briefly touched on 95% confidence intervals, sometimes described as 95% CI, when we discussed measuring variability in your data. [See here for a reminder](https://naturalandenvironmentalscience.shinyapps.io/variation/#section-confidence-intervals). One of the disadvantages of 95% CI is that they are often misunderstood, even by scientists with many years of experience. Try out this little quiz; **Hint**: I tried it once with my colleagues, and most of them gave the wrong answer!

```{r interpret95ci}
question("You take 6 randomly placed vegetation quadrats in a hay meadow, 1 m-square, and count the number of flowering plant species in each quadrat. The average number of species per m-squared is 13.0, with a 95% CI of +/- 2.5 . Which of the following statements is true?",
         answer("Additional vegetation quadrats should have been taken"),
         answer("There is a 95% probability of the mean number of species being between 10.5 and 15.5"),
         answer("If an additional 6 quadrats are collected there is a 95% probability that the mean number of species will be between 10.5 and 15.5"),
         answer("If one additional quadrat is surveyed there is a 95% probability that the mean number of species per m-squared is between 10.5 and 15.5"),
         answer("If you repeated the survey 20 times, there is a 95% probability that the mean number of species per m-squared would be between 10.5 and 15.5 on 19 of the surveys", correct=TRUE),
         answer("If you take 5 additional quadrats, the mean number of species per metre should be 13.0 with a probability of 0.95")
)
```

If you are like most people you may have ticked the wrong answer, and the correct one is a bit of a surprise. Most people incorrectly interpret 95% as **Bayesian credible intervals**, in that they make the mistake of assuming that there is a 95% probability that the mean value lies between the upper and lower bands. However they are actually **95% confidence intervals** which relates to what the average is in a set of **samples**, **populations** and **effect sizes**. Before going any further, it will be useful to define some of these terms.

## Populations and samples
These two terms are used by statistians in any discipline of science, to refer to particular statistical concepts. However, as ecologists we're used to thinking about populations as being a 'population of animals' etc., and so it is easy to get confused by the statistical geekdom. Let's go back to our hay meadow, where we want to work out the mean number of species per $m^2$. Here's a representation of your meadow:

![](www/meadow.png)

As you might expect, some parts of your meadow have lots of species of flowering plants present, others parts relatively few and dominated by grasses. So what is the mean number of flowering plants per $m^2$ in your meadow? There are two approaches:

* get down on your hands and knees, and spend a whole week surveying every inch of the meadow, and at the end of the week divide the number of flowering plants by the area of the meadow (assuming you are still capable of standing up). This is the **population mean**
* randomly place 6 vegetation quadrats, each $1m^2$ in your meadow, and calculate the average number of flowering plants from your quadrats. This is the **sample mean**.

In many, if not most situations, it is actually impossible to determine the population mean reliably, hence we use sample means. The problem with sample means is that because of their inherent randomness, such as our 6 randomly placed quadrats, you will get a slightly different result each time. The more quadrats you take, the smaller the 95% CI around your sample mean, but there will still be variability.

## Effect sizes
### What are effect sizes and why do they matter?
When you present the results of a study to policy-makers, you could say something like:

* "Our new low-intensity management regime on haymeadows was amazingly successful and increased the number of flowering plants and was highly significant (F=6.87, p<0.001)"

or you could say:

* "Our low intensity management regime increases overall farmer costs by £6/ha and resulted in an increase in the numbers of flowering plants from 13.0$m^2$ to 25.8$m^2$

Which is better? The second not only is much more neutral and professional in its wording, but also explains clearly the costs and benefits of the new low intensity management regime. A policy maker will care much more about these issues than an F-ratio and p-value from a statistical test.

### Problems with mis-use of NHST and p-values
A major issue with p-values is that on there own they do not give any indication of the magnitude of an experimental effect (or even the direction positive or negative). There is also an issue if the 'traditional' 0.05 cut-off is treated to strictly that it can lead to subjective biases in reporting...

![](www/significance.png)

Let me stress that there is nothing inherently wrong with p-values; indeed they are lurking in most of my papers somewhere. It is their mis-use that can cause problems. Given a very large sample size it is possible to obtain highly significant results that are of little real-world interest. For example, suppose you survey a conventionally managed and organically haymeadows with a huge 50 vegetation quadrats each. You could end up with p=0.0000004534 which is massively significant, but if the mean number of species of plants $m^2$ only went up from 13.0 to 13.8 it would not be of practical value. Also remember that 0.05 is arbritary, so even if your results are "non-significant" they should still be reported, with the p-value given in full.

Therefore, when reporting p-values, it is essential to record some measure of variation (sd, se or 95% CI), the number of replicates, and also the effect size. This area is quite contentious. I suggest you look at [this paper in Conservation Biology](https://conbio-onlinelibrary-wiley-com.libproxy.ncl.ac.uk/doi/full/10.1111/j.1523-1739.2006.00525.x) for criticism of null hypothesis significance tests and p-values, and also [this one from Ecology](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/13-0590.1?casa_token=GLWn_k3GHQEAAAAA%3AW2ZcXhWsih4A3Y9ZmOpP40cpyfJiq2gU6tOb6NaVNb-JRTS9xw7yv3hO7Indl98gz3X3QTySlupKnw) defending their use.

You might also find [this video](https://www.youtube.com/watch?feature=player_embedded&v=ez4DgdurRPg) of dancing p-values from Geoff Cummings entertaining and informative. There is also [this interactive](https://rpsychologist.com/d3/ci/) on confidence intervals and p-values helpful to your understanding.

## How to measure effect sizes
We will cover these in much more detail later, and discuss how to convert between effect sizes, so this page just gives you an overview of some of the main types. There are actually a large numbers of different methods of measuring effect sizes, depending on the characteristics of your data. Effect sizes are designed to put the outcomes of the different studies being combined on the same scale, so that they can be incorporated into your meta-analysis. Some of the most common ones are:

### Effect sizes based on means
Raw unstandardised mean differences (D) are generally of little value as they do not correct properly for variability  in your data. So the most common approach is **standardised mean differences** especially **Hedges d or g** measures. Another effect size based on means is **response ratio** calculated by dividing one mean by another. As soon as you work with ratios, it is generally better to use logarithms.

For example, suppose you have a mean of 5.3 for one intervention, and a mean of 8.7 for the other. Quite obviously your ratios will be quite different if you divide them in different orders, but if you take logarithms, the sign changes, but the magnitude remains constant:

```{r response_ratio, exercise=TRUE}
# Original data; order of calculation of ratio matters
5.3 / 8.7
8.7 / 5.3

# Log response ratio and only the sign changes but magnitude consistent
log(5.3 / 8.7)
log(8.7 / 5.3)
```

### Effect sizes based on binary data
Binary data, also known as dichotomous or frequency data. These types of data are often encountered in ecological surveys, for example whether an animal is ‘dead’ vs ‘alive’ or ‘present’ vs ‘absent’, or 'healthy' vs 'unhealthy'. The two main methods are **risk ratios** and **odds ratios**

* **risk ratios** This is the ratio of two risks (as the name suggests). So e.g. if the risk of a vaccinated badger having bovine TB is 5 in 100 treated badgers, and the risk of an unvaccinated badger has bovine TB is 10 in 100 untreated badgers, then the risk ratio is 5/100 / 10/100 = 0.50. Again, we generally take logarithms.

$$\frac{5\div100}{10\div100}=0.5$$

* **odds ratios** Taking our badger example again it is a slight re-working of the original numbers. Again, the ratios are usually then converted to logarithms:

$$\frac{5\div(100-5)}{10\div(100-10)}=\frac{5\div95}{10\div95}=0.4737$$


### Effect sizes based on correlations
If two continuous variables have a high correlation with each other the scatter of points will generally create a straight line, increasing with a positive correlation, and decreasing with a negative correlation. If the scatter of points is random, the correlation is zero, with the correlation coefficient, r, measured between -1 and +1. [This interactive](https://rpsychologist.com/correlation/) gives a very nice summary of correlation. Correlation data are widely reported in the ecological literature.

However, in general for meta-analysis, we do not work with the raw correlations, but instead convert them to **Fisher's _z_ scores**. Note: do not confuse this with the z-statistic sometimes used in statistical tests.

## A simple example of meta-analysis
This uses the example taken from Chapter 1 of Borenstein. It is based on 4 trials of statins to see if they reduce the chance of heart disease. It is based on risk-ratios.

The raw data can be accessed in the R data.frame below. The columns represent:

* nt = number of patients treated with statins
* nc = number of patients with control dose
* ep1t = number of treated patients who suffered heart disease
* ep1c = number of control patients who suffered heart disease

```{r borenstein_chap01-setup}
library(metafor)
statin_dat <- dat.cannon2006[,c(1,3:6)]
```

```{r borenstein_chap01, exercise=TRUE}
statin_dat
```

These data can then be used to construct the following plot, which is shown on Page 4 of Borenstein as Figure 1.1 However, this version has an advantage over the one in the book. You can alter the effect sizes (risk ratios)! 

```{r, echo = FALSE}
    dat <- dat.cannon2006[,1:6]
    # Application title
    #titlePanel("How a meta-analysis works")
    plotOutput("metaPlot")
    
    fluidRow(
        column(4,
               h4("PROVE IT")
               ),
        
        column(8,
               sliderInput("yi_1",
                           label = NULL, 
                           min = 0.5,
                           max = 1.5,
                           value = 0.84)
               )
        )
        
        fluidRow(
            column(4,
                  h4("A-TO-Z")
            ),

            column(8,
                   sliderInput("yi_2",
                               label = NULL, 
                               min = 0.5,
                               max = 1.5,
                               value = 0.860)
            )
        )
    
    fluidRow(
        column(4,
               h4("TNT")
        ),
        
        column(8,
               sliderInput("yi_3",
                           label = NULL, 
                           min = 0.5,
                           max = 1.5,
                           value = 0.801)
        )
       
    )
    fluidRow(
        column(4,
               h4("IDEAL")
        ),
        
        column(8,
               sliderInput("yi_4",
                           label = NULL, 
                           min = 0.5,
                           max = 1.5,
                           value = 0.890)
        )
      
    )
    
    actionButton("reset", "Reset values")

```
```{r, context="server"}
    observeEvent(input$reset, {
        # Reset values back to original
        updateSliderInput(session, "yi_1", value = 0.840)
        updateSliderInput(session, "yi_2", value = 0.860)
        updateSliderInput(session, "yi_3", value = 0.801)
        updateSliderInput(session, "yi_4", value = 0.880)
    })

    output$metaPlot <- renderPlot({

        dat <- escalc(measure="RR", ai=ep1t, n1i=nt, ci=ep1c, n2i=nc, data=dat, slab=trial)
        
        # Modify effect sizes (which are Risk Ratios so have to be logs)
        dat[1, "yi"] <- log(input$yi_1)
        dat[2, "yi"] <- log(input$yi_2)
        dat[3, "yi"] <- log(input$yi_3)
        dat[4, "yi"] <- log(input$yi_4)
        
        res <- rma(yi, vi, data=dat, method="DL")
        dat$weights <- paste0(round(weights(res)), "%")   # weights in % (rounded)
        dat$pvals   <- round(summary(dat)$pval, digits=3) # p-values of the individual trials
        forest(res, xlim=c(-1,2), atransf=exp, at=log(c(2/3, 1, 3/2)),
               header=TRUE, top=2, mlab="Summary", efac=c(0,1,3),
               ilab=data.frame(dat$weights, dat$pvals), ilab.xpos=c(0.8,1.2), ilab.pos=2)
        text(0.8, -1, "100%", pos=2)
        text(1.2, -1, formatC(res$pval, format="f", digits=5), pos=2)
        text(0.8,  6, "Weight",  pos=2, font=2)
        text(1.2,  6, "P-Value", pos=2, font=2)
        
    })

```

In the summary or **forest plot** the larger the symbol for each study the greater its weight in the meta-analysis. The diamond symbol shows the overall effect size for all studies, adjusting for weights, with the length of the diamond indicating the variability across the studies.

```{r interpret_basic_meta}
question("Altering the effect size (risk ratios) of the 'PROVE IT' and 'A-TO-Z' trials has a much smaller effect than changing the risk ratios for the 'INT' and 'IDEAL' trials. What is the most likely explanation?",
         answer("The PROVE IT and A-TO-Z trials were done on different cohorts of patients"),
         answer("The time used in each set of studies was not the same"),
         answer("More patients had heart disease in these studies"),
         answer("The INT and IDEAL trials had more than twice as many patients", correct = TRUE),
         answer("The 95% confidence intervals are more than twice that for the other studies"))
```

